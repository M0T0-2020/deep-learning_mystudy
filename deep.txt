Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 05:52:31) 
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin
Type "copyright", "credits" or "license()" for more information.
>>> import torch.nn
>>> m = nn.Linear(20,30)
Traceback (most recent call last):
  File "<pyshell#1>", line 1, in <module>
    m = nn.Linear(20,30)
NameError: name 'nn' is not defined
>>> m = Linear(20,30)
Traceback (most recent call last):
  File "<pyshell#2>", line 1, in <module>
    m = Linear(20,30)
NameError: name 'Linear' is not defined
>>> import torch.nn.LInear
Traceback (most recent call last):
  File "<pyshell#3>", line 1, in <module>
    import torch.nn.LInear
ModuleNotFoundError: No module named 'torch.nn.LInear'
>>> import torch.nn.Linear
Traceback (most recent call last):
  File "<pyshell#4>", line 1, in <module>
    import torch.nn.Linear
ModuleNotFoundError: No module named 'torch.nn.Linear'
>>> from torch.nn import Linear
>>> m = Linear(20, 30)
>>> m
Linear(in_features=20, out_features=30, bias=True)
>>> m.add_module
<bound method Module.add_module of Linear(in_features=20, out_features=30, bias=True)>
>>> m.add_module()
Traceback (most recent call last):
  File "<pyshell#9>", line 1, in <module>
    m.add_module()
TypeError: add_module() missing 2 required positional arguments: 'name' and 'module'
>>> m
Linear(in_features=20, out_features=30, bias=True)
>>> import torch
>>> torch.randn(30,100)
tensor([[-2.8697e-01, -1.4010e+00,  1.8724e+00,  ...,  1.0674e+00,
         -1.3616e-01, -1.8728e+00],
        [ 5.0201e-01, -5.7787e-01, -1.6089e-01,  ..., -1.4841e+00,
          1.3338e+00, -2.0517e+00],
        [-1.0728e+00,  1.4898e+00, -1.2693e-03,  ...,  1.7629e+00,
         -2.0548e-02,  6.0721e-01],
        ...,
        [ 1.9564e-01,  1.8253e+00, -1.6843e-01,  ...,  1.1429e+00,
          3.1215e-01,  8.3291e-01],
        [-1.7281e+00,  1.6948e+00, -2.5353e+00,  ..., -1.2398e+00,
          7.8667e-02,  2.2927e-02],
        [ 9.0052e-01, -1.8030e-01,  3.5481e-01,  ..., -7.6655e-01,
          6.6298e-01, -1.1533e+00]])
>>> tensr = torch.randn(30,100)
>>> m(tensr)
Traceback (most recent call last):
  File "<pyshell#14>", line 1, in <module>
    m(tensr)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 92, in forward
    return F.linear(input, self.weight, self.bias)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py", line 1406, in linear
    ret = torch.addmm(bias, input, weight.t())
RuntimeError: size mismatch, m1: [30 x 100], m2: [20 x 30] at ../aten/src/TH/generic/THTensorMath.cpp:961
>>> tensr = torch.randn(100,30)
>>> m(tensr)
Traceback (most recent call last):
  File "<pyshell#16>", line 1, in <module>
    m(tensr)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 92, in forward
    return F.linear(input, self.weight, self.bias)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py", line 1406, in linear
    ret = torch.addmm(bias, input, weight.t())
RuntimeError: size mismatch, m1: [100 x 30], m2: [20 x 30] at ../aten/src/TH/generic/THTensorMath.cpp:961
>>> tensr = torch.randn(20)
>>> m(tensr)
tensor([-0.0432, -0.2489, -0.1031,  0.0711, -0.9278, -0.3236, -1.0094, -0.5122,
        -0.1303, -0.3162,  0.7963,  0.8340,  0.8109,  0.3592,  0.1283, -0.8575,
         0.7360,  0.5659,  0.8454,  0.0248,  0.6552, -0.3185,  0.8520,  0.6954,
         1.0630,  0.1039, -1.8651,  0.0113,  1.1503,  0.3024],
       grad_fn=<AddBackward0>)
>>> m
Linear(in_features=20, out_features=30, bias=True)
>>> out = m(tensr)
>>> len(our)
Traceback (most recent call last):
  File "<pyshell#21>", line 1, in <module>
    len(our)
NameError: name 'our' is not defined
>>> len(out)
30
>>> tensr = torch.randn(20,10)
>>> out = m(tensr)
Traceback (most recent call last):
  File "<pyshell#24>", line 1, in <module>
    out = m(tensr)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 92, in forward
    return F.linear(input, self.weight, self.bias)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py", line 1406, in linear
    ret = torch.addmm(bias, input, weight.t())
RuntimeError: size mismatch, m1: [20 x 10], m2: [20 x 30] at ../aten/src/TH/generic/THTensorMath.cpp:961
>>> tensr = torch.randn(20,30)
>>> out = m(tensr)
Traceback (most recent call last):
  File "<pyshell#26>", line 1, in <module>
    out = m(tensr)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 92, in forward
    return F.linear(input, self.weight, self.bias)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py", line 1406, in linear
    ret = torch.addmm(bias, input, weight.t())
RuntimeError: size mismatch, m1: [20 x 30], m2: [20 x 30] at ../aten/src/TH/generic/THTensorMath.cpp:961
>>> tensr = torch.randn(30,20)
>>> out = m(tensr)
>>> oyt
Traceback (most recent call last):
  File "<pyshell#29>", line 1, in <module>
    oyt
NameError: name 'oyt' is not defined
>>> out
tensor([[ 1.4340e-01,  8.7052e-01,  3.1787e-03,  1.3852e-01,  6.2875e-02,
          1.0598e+00, -2.1396e-01, -5.1129e-01,  6.9125e-01,  1.8585e-02,
         -9.1891e-01,  9.2800e-01,  1.8997e-01, -1.0653e+00,  1.4914e+00,
          2.7435e-01,  1.1684e+00,  4.8510e-02,  1.2157e-01,  9.5668e-01,
         -1.6255e+00, -7.3457e-02, -1.0172e+00, -7.8995e-01,  8.4268e-01,
         -1.7200e-01,  1.7427e-01,  6.2890e-01, -5.5980e-01, -4.7275e-01],
        [-8.6002e-01, -1.4305e+00, -5.1740e-01,  1.0144e+00,  5.8103e-01,
          3.7107e-01, -6.1399e-01,  7.5944e-01,  5.9468e-02, -6.2956e-01,
         -9.5344e-01, -1.6412e-01, -3.4283e-01, -5.4162e-01,  7.2254e-01,
          1.4604e-01, -5.1688e-01,  1.2767e-01, -2.4158e-02, -4.2227e-02,
          7.5090e-02, -4.3649e-01,  9.1034e-03, -8.0078e-01,  8.2805e-01,
         -2.0107e-01, -7.8494e-01, -1.7053e-01,  2.4635e-01, -5.4640e-01],
        [-3.4216e-01, -1.3057e+00,  1.6795e-01,  1.7637e-01, -4.1695e-01,
          3.0319e-01, -7.8467e-01, -4.7876e-01, -7.8565e-01, -2.2187e-01,
          1.8788e-01, -2.3646e-01,  6.2390e-01,  5.0060e-01, -3.3309e-01,
         -5.4875e-01, -4.7528e-01, -4.2561e-01,  6.0584e-01, -8.0983e-01,
          7.8933e-01, -5.1840e-01,  4.2638e-01, -5.8994e-01,  1.2946e+00,
         -7.8971e-01, -1.0514e+00,  1.1508e-01,  7.7355e-01, -6.6131e-01],
        [ 8.3101e-01, -1.1919e+00, -4.5153e-01, -4.2126e-01, -3.5215e-01,
          5.0104e-01,  2.3388e-01, -5.8286e-01,  7.9649e-02,  1.2902e-01,
         -3.0140e-01, -1.0668e+00, -3.6583e-01, -4.8302e-01,  2.6337e-01,
         -3.8827e-01, -6.4151e-01, -6.3112e-02, -4.4755e-01,  6.0554e-03,
         -6.4625e-01,  5.7513e-01, -1.1033e+00, -1.8828e-01, -3.1189e-01,
          7.1954e-01, -3.8187e-01,  2.1136e-01,  5.4759e-01, -4.6866e-01],
        [-9.3677e-01, -1.4739e+00, -7.3880e-01, -2.0874e-01, -4.0504e-01,
          6.3368e-02, -1.8164e-01,  7.3468e-01, -6.4133e-01, -7.3683e-01,
         -1.9320e-01, -2.6482e-01, -1.3272e-01, -1.6353e-01, -4.6695e-01,
          3.6531e-01,  4.5656e-01,  3.8212e-01, -5.5010e-01, -5.9545e-02,
          2.4791e-01,  3.8653e-01,  2.2467e-01,  1.1995e+00, -8.3434e-01,
          4.8025e-01, -8.6357e-02, -9.4066e-01,  3.9732e-01,  4.2610e-01],
        [ 6.5595e-01,  1.8565e-01,  4.9075e-01, -2.9573e-02, -7.8425e-02,
         -1.3958e-01,  7.4320e-01, -1.1855e-01, -3.2272e-01,  3.5298e-01,
          2.9288e-01, -1.2639e-01, -1.7094e-03, -1.4901e-01, -3.2859e-01,
          1.0077e+00, -5.1549e-01,  4.3816e-01, -1.2638e-01,  3.7016e-03,
         -3.9766e-01,  4.1780e-01,  6.1701e-01,  3.2862e-01, -4.9151e-01,
          1.9512e-01,  4.9128e-01, -3.9624e-01, -5.4900e-01, -8.3409e-01],
        [ 8.9781e-02, -5.8962e-01, -4.1213e-01, -4.9596e-01, -1.3392e-02,
          1.7551e-01,  7.6172e-02, -3.1346e-01,  4.2928e-01, -4.0730e-01,
         -4.4344e-01, -6.4232e-01, -5.0814e-01, -2.2069e-01, -6.5519e-01,
          1.8822e-01, -3.3450e-01,  3.0689e-02, -6.2806e-01,  3.6298e-01,
         -4.6103e-01,  2.1720e-01, -7.0169e-01,  7.2220e-02, -1.0097e+00,
          1.7796e-01, -4.3216e-02, -3.1921e-01,  3.4312e-01,  1.7882e-01],
        [-5.0997e-01, -3.1917e-01,  6.6723e-01,  2.0975e-01, -1.6741e-01,
          4.6314e-01, -7.6634e-01,  3.2709e-01, -8.0308e-01,  6.0053e-01,
          3.9405e-01,  9.0765e-01,  4.9573e-01, -4.9132e-02,  5.1240e-01,
          9.7987e-02,  2.7744e-01,  4.4925e-02,  5.8186e-02, -7.6165e-02,
          8.3444e-01, -6.5237e-01,  7.2013e-01, -9.4151e-02,  6.8929e-01,
          6.2218e-02, -1.0148e+00,  2.8796e-01,  5.2269e-01, -3.2301e-01],
        [ 5.5739e-01, -1.0414e+00, -6.3992e-01, -1.8234e+00, -9.0826e-01,
          2.8249e-02, -5.2909e-02, -1.2043e+00, -8.4714e-01, -3.3629e-01,
          2.5153e-01, -1.5012e+00, -1.9251e-01, -2.1896e-02, -1.6196e+00,
         -3.8690e-01, -2.5472e-02,  2.7783e-02, -6.5954e-01, -2.8240e-01,
         -1.5516e-01,  1.0628e+00, -1.1724e+00,  4.4739e-01, -1.1959e+00,
         -1.1613e-01,  4.1914e-02, -6.0858e-01,  5.3546e-01, -2.8884e-02],
        [ 1.7916e-01, -2.6095e-02,  5.1009e-01,  3.4692e-01,  5.8737e-01,
          4.9292e-01,  3.2428e-01,  5.3108e-01,  3.7460e-01,  1.7480e-01,
         -7.3071e-01,  1.6082e-01,  3.1822e-02, -1.0458e+00, -2.9715e-01,
          5.7123e-01, -1.0120e+00,  1.6809e-01,  2.2386e-01,  2.5889e-01,
         -8.8779e-01, -4.6707e-02,  6.7313e-01,  7.0225e-01, -3.7062e-01,
          1.0786e-01,  6.7892e-01, -1.2863e-01,  1.3550e-01, -6.2450e-01],
        [ 4.1190e-01,  3.1000e-01,  4.9239e-01,  2.1840e-02, -1.8330e-01,
          3.8874e-01,  3.4156e-02, -2.8505e-01,  1.1036e-01,  6.1755e-01,
         -1.8701e-02,  1.3611e-01, -1.3560e-01, -1.2232e-01,  3.1979e-01,
          4.1086e-01, -2.5028e-01, -1.9990e-01, -1.1386e-01,  4.7196e-01,
         -1.7325e-01, -2.5667e-01, -6.0753e-01, -4.5691e-01,  3.9118e-01,
          2.6702e-01, -1.1909e-02,  1.3867e-01,  1.4382e-01, -3.8849e-01],
        [ 5.9749e-01, -1.3696e+00,  2.5751e-01, -1.1068e-01, -4.2594e-01,
         -1.6779e+00,  8.4458e-02, -5.1257e-01,  1.8835e-01,  3.6418e-01,
          6.3749e-01, -1.5643e+00, -1.6246e-01,  3.1076e-01,  7.5956e-01,
         -2.3844e-01, -1.1080e+00, -3.1756e-02, -2.4859e-01, -9.1765e-01,
          8.0097e-01,  1.4742e-01,  4.2006e-01, -3.8765e-02,  3.5530e-01,
          3.0879e-01, -4.1703e-01, -9.5254e-01, -3.2389e-01, -5.6246e-01],
        [ 9.9434e-01,  2.2954e-01,  2.0539e-01,  4.5181e-01, -2.7669e-01,
          7.2327e-01, -5.6798e-01, -6.2200e-01, -2.6762e-01,  3.2246e-01,
         -5.1555e-01, -6.4497e-02, -4.6432e-01,  1.6646e-02, -1.3312e-01,
         -8.9563e-01, -1.6207e-01,  1.8506e-01,  4.9762e-01,  7.9169e-01,
         -1.5608e-01, -1.9975e-01, -1.1025e+00,  4.0262e-01, -7.4791e-01,
         -5.1956e-01, -5.5935e-01, -7.3748e-01,  4.3145e-01, -3.5886e-01],
        [-8.0098e-01,  7.7558e-01, -3.2207e-02,  2.0989e-01, -4.2953e-01,
          5.1763e-02,  1.2025e-01, -7.6188e-01,  2.6904e-01, -4.4523e-01,
          1.9213e-01,  3.8535e-01,  4.5791e-01, -1.6466e-04, -2.2598e-01,
          4.2346e-01, -4.6807e-02,  2.5918e-01, -2.0908e-01,  5.2442e-01,
         -4.0734e-01, -6.3462e-01,  8.7758e-01, -2.5785e-01,  1.1080e+00,
          4.0790e-01, -7.7598e-01,  1.5909e-01,  4.8918e-01,  1.2066e+00],
        [-6.3022e-01,  1.1608e-01,  3.0301e-01, -1.6441e-01,  6.1857e-01,
         -9.3975e-01, -1.0598e+00,  1.7755e-01, -1.0605e-01,  2.5021e-02,
          5.6465e-01, -3.7458e-01, -1.0631e-01, -4.2532e-01, -1.7191e-01,
         -1.2849e-01, -3.5947e-01, -4.8875e-01,  6.3144e-02, -7.7048e-01,
          8.7984e-01, -8.6403e-01,  3.5922e-01, -1.1966e-01,  7.1742e-01,
         -5.8549e-01,  1.3336e-01, -5.5924e-01,  6.7259e-02, -3.7761e-01],
        [-1.0788e-01,  4.7518e-01,  1.1276e-01,  2.6817e-01,  1.1972e-02,
         -6.6285e-01, -6.8139e-01, -1.1945e+00, -7.4380e-01, -2.9891e-01,
         -1.6015e-01, -9.2397e-01,  1.2038e-01,  5.6889e-01, -2.5568e-01,
         -9.8487e-01, -2.8430e-01, -2.6743e-01,  2.2935e-01, -3.3794e-01,
          4.3341e-01, -9.4861e-01,  1.2254e-02,  3.7148e-01,  2.5304e-01,
         -2.5000e-01, -1.2130e+00, -1.2100e+00,  1.0041e+00,  1.7696e-01],
        [-7.1869e-01, -6.0597e-01,  9.3909e-01, -4.7497e-01,  2.2194e-01,
         -2.1886e-01, -7.3799e-01,  3.7676e-01,  1.0994e+00, -1.0808e-01,
          1.3000e-01,  8.5752e-01,  5.7622e-01,  1.4005e+00, -5.3263e-01,
          1.4199e+00,  4.7860e-01, -1.1531e+00, -3.1516e-02, -8.6483e-01,
          1.2567e+00, -4.5124e-01,  3.4045e-01, -5.5364e-02,  2.5615e-01,
         -4.6562e-01,  7.8448e-01, -3.4742e-01, -1.2813e-01, -9.7495e-01],
        [ 2.0893e-01,  7.3600e-02,  5.4203e-01, -1.3429e-01, -5.5476e-01,
         -8.6842e-02, -5.4831e-01,  2.9899e-01, -4.3434e-01,  8.2406e-01,
          1.1064e+00,  6.3954e-01,  7.4533e-01, -9.4879e-01,  9.8363e-01,
          4.1061e-02,  3.1108e-01,  1.4812e-01,  1.8979e-01,  5.4111e-02,
          2.4030e-01, -5.6951e-02,  7.4945e-01,  5.1009e-01,  8.9035e-01,
          6.2589e-01, -5.2118e-02,  4.3482e-01,  2.7546e-02,  1.7008e-01],
        [-4.7777e-01, -7.4659e-01,  2.5669e-02, -7.8885e-02,  8.6674e-02,
          3.6368e-01, -3.8017e-01,  3.1937e-01,  2.9299e-01,  2.1969e-01,
         -2.8815e-01, -4.5434e-01, -3.0339e-01,  5.0862e-01,  4.9958e-01,
          6.3348e-01, -2.6267e-01, -9.1249e-02, -1.9808e+00,  2.9107e-01,
          9.9962e-01, -2.8408e-01, -3.3687e-01, -1.5749e+00,  5.4814e-01,
          5.8540e-02, -1.1617e+00, -5.7665e-01,  3.0336e-01,  3.2984e-01],
        [-3.6941e-02, -1.2020e+00,  2.1173e-01, -1.1510e-01, -6.2841e-01,
          9.7580e-01, -1.8969e-01, -6.2699e-01,  4.9809e-01,  3.1018e-01,
         -2.2601e-01, -3.7911e-01, -2.6652e-01, -3.2468e-01, -1.0603e+00,
         -4.0963e-01, -1.2495e+00,  3.2718e-01, -1.1876e-01,  2.9425e-01,
         -4.2561e-01, -3.1343e-01, -7.4402e-02, -2.7715e-01, -1.2735e-01,
         -4.2329e-01, -7.4791e-03,  5.7298e-01, -4.0350e-01,  8.2839e-01],
        [ 3.9633e-01,  1.1375e+00,  8.3976e-01,  3.0494e-01,  5.5444e-01,
         -3.7061e-01, -2.7516e-01, -2.3758e-01, -7.6917e-02,  1.6592e-01,
         -3.7985e-02,  6.8623e-01,  9.1149e-01, -1.0539e+00,  5.1677e-01,
          1.2008e+00, -7.2649e-01,  4.3203e-01,  4.7379e-01,  4.4774e-01,
         -2.6974e-01,  4.3818e-01,  1.4935e+00,  4.3273e-01,  5.3331e-02,
          7.1865e-01, -1.1119e-01, -3.1026e-01,  3.4213e-01, -3.7422e-01],
        [ 1.1356e-01, -2.5717e-01,  5.8897e-01, -2.0488e-01, -1.0176e+00,
          1.0912e+00,  3.7283e-01, -4.8793e-01, -8.4198e-01,  5.2595e-01,
          9.3876e-01,  2.7206e-01,  1.2150e+00,  5.0770e-01, -8.8112e-01,
          9.4304e-01, -3.9237e-01,  2.6356e-01, -5.9017e-01, -1.7699e-01,
         -3.3613e-02,  3.5561e-02,  1.2444e+00, -1.1693e-01,  8.1799e-01,
          6.9525e-01, -2.2992e-01,  8.6614e-01, -3.8506e-02,  7.6415e-01],
        [-8.1365e-02,  1.3882e+00,  5.5979e-01,  3.9484e-01,  4.5968e-01,
         -2.0946e-01, -2.5412e-01,  2.7787e-01,  5.5879e-01,  1.1179e-01,
          3.1295e-03,  9.6882e-01, -3.8291e-01, -1.3087e-01, -1.1021e+00,
          7.4227e-01, -1.1404e-01,  7.3884e-01,  6.1896e-01,  2.9353e-01,
         -1.2312e-01, -4.0634e-01,  6.7806e-01,  1.4472e+00, -3.9567e-01,
          3.9875e-02,  6.6076e-01, -9.0931e-01, -2.9863e-01, -3.9222e-01],
        [-1.3505e-02,  4.8907e-01, -9.9744e-02,  7.8044e-01,  6.3087e-01,
         -6.9733e-01, -1.2566e-01, -4.2047e-03, -3.2844e-01, -1.9586e-01,
         -1.0863e+00, -5.8628e-01, -6.9373e-01, -8.5766e-01, -1.0859e-01,
          2.1470e-01, -1.5778e+00,  1.2231e+00, -1.3382e-01,  1.1973e+00,
         -4.9832e-02, -1.6245e-01,  3.9783e-01,  3.8738e-01, -3.8490e-01,
          8.4447e-01, -1.6443e+00, -1.3924e+00,  1.4265e+00,  3.2444e-01],
        [ 2.5267e-01,  3.0981e-01, -1.2958e-01,  6.6477e-01, -2.1783e-01,
         -1.7582e-01, -7.4066e-01, -9.8330e-01, -5.3448e-01, -5.9159e-01,
         -1.6922e-01, -4.8751e-01,  9.9359e-01, -9.7564e-03, -1.7866e-01,
         -3.2436e-01, -1.7698e-01,  4.9582e-01,  5.8676e-01, -9.3807e-02,
         -5.5015e-01, -3.6357e-01,  6.6571e-01,  6.4994e-01,  7.3948e-01,
          2.2373e-01, -6.2773e-01, -4.4009e-01,  5.4715e-02,  8.6706e-01],
        [ 8.4435e-01, -4.2036e-01, -1.8401e-01,  6.0971e-01, -8.3495e-01,
          1.4666e-01, -6.8364e-01, -8.7566e-01, -4.3549e-01, -7.0470e-01,
          4.8315e-01,  3.2949e-01,  7.7167e-01,  3.3541e-01, -5.7109e-01,
         -3.4790e-01, -1.4741e-01,  6.6728e-01,  1.1872e+00,  1.6502e-01,
          1.7629e-01,  7.7485e-01,  9.4135e-01,  1.4598e+00, -9.0155e-01,
          1.2857e-01, -7.4999e-01, -7.0535e-01,  4.3933e-01,  2.8849e-01],
        [ 1.1405e-02, -2.8299e-02, -2.4732e-01, -1.5285e-01, -6.2232e-01,
         -3.0901e-01, -5.7954e-01, -1.4135e+00, -1.2112e-01, -4.9992e-01,
          5.1637e-01, -2.7159e-01,  8.8648e-01, -1.7636e-01,  8.5696e-01,
         -1.8001e-01,  7.1742e-01, -4.3826e-01,  5.3766e-01, -5.7711e-01,
         -5.0971e-01,  1.6776e-01, -3.3328e-02,  1.5234e-01,  7.4507e-01,
          3.5774e-01,  1.9823e-01,  1.3007e-01, -4.6218e-01,  2.5846e-01],
        [-9.4955e-01, -7.8756e-02,  2.5779e-01,  2.4842e-01,  7.7046e-01,
         -6.6653e-01, -4.7522e-01,  3.0096e-01,  2.3146e-01, -3.6769e-01,
         -1.8469e-01,  3.5256e-01,  1.2529e-01, -1.1004e+00,  3.8489e-01,
          1.4530e-01, -4.3377e-01,  2.2303e-01,  3.7933e-01, -4.6579e-01,
          1.1986e-01, -2.1564e-01,  1.6213e+00,  6.1160e-01,  1.5569e-01,
         -2.2232e-01, -1.9875e-01, -6.6319e-01,  2.1000e-01, -6.7539e-01],
        [-3.4159e-02,  5.3500e-03,  1.0448e-01,  4.4199e-01,  4.9504e-01,
          5.9115e-01,  2.9802e-01, -2.7660e-01, -6.5110e-01, -1.7364e-02,
         -4.2812e-01, -7.2973e-01, -7.8398e-01,  3.8959e-01, -7.5336e-01,
          3.7414e-02, -4.2622e-01, -3.9720e-01, -3.3311e-01, -3.1413e-01,
         -3.1180e-01, -7.5574e-01, -5.9505e-01, -1.1128e-01, -9.6670e-01,
         -5.6674e-01,  3.0991e-01, -5.6768e-01, -3.0566e-01, -7.5011e-01],
        [-1.7973e-02, -6.3652e-01,  5.9894e-01, -9.6255e-01, -9.9090e-01,
         -1.6134e-01, -1.4752e-02, -7.3324e-01, -8.0715e-01,  1.2524e+00,
          1.5397e+00, -4.1418e-02, -4.7078e-01, -9.5509e-02,  5.1815e-01,
         -3.3763e-01,  3.5392e-01, -4.4464e-01, -3.2214e-01, -3.0882e-01,
          6.3441e-01, -3.3428e-01, -4.9279e-01, -1.2527e-01, -7.5595e-01,
          3.7219e-01, -6.5945e-02,  7.3791e-01, -4.4833e-01, -1.0184e-01]],
       grad_fn=<AddmmBackward>)
>>> m
Linear(in_features=20, out_features=30, bias=True)
>>> m
Linear(in_features=20, out_features=30, bias=True)
>>> m.weight
Parameter containing:
tensor([[-0.0557,  0.1514,  0.2111, -0.0131, -0.1602, -0.0141, -0.1485,  0.0108,
          0.0027,  0.1792, -0.0073,  0.0699, -0.1367,  0.2036, -0.0046, -0.0535,
          0.2128, -0.1932, -0.0833, -0.0161],
        [-0.0935, -0.1975,  0.2056, -0.2231, -0.1726, -0.1535,  0.0663, -0.1838,
         -0.1285, -0.0928, -0.1453,  0.0816, -0.0933, -0.0452, -0.1915, -0.0703,
          0.1623,  0.0109, -0.1940, -0.2166],
        [ 0.0103,  0.0252,  0.1724,  0.0103, -0.0388,  0.0422, -0.0869, -0.1995,
         -0.0747, -0.1896, -0.0341,  0.0067, -0.1959,  0.0094, -0.0094,  0.0416,
         -0.0668, -0.1378,  0.0858, -0.1142],
        [ 0.0571,  0.0324,  0.1022,  0.0821, -0.0101, -0.1495,  0.0486, -0.1129,
         -0.0617,  0.1359, -0.0642,  0.1488, -0.0331, -0.0181, -0.2230,  0.1914,
          0.1736,  0.1218,  0.1150, -0.0587],
        [ 0.1897,  0.0572, -0.1503, -0.0117, -0.1099, -0.1602,  0.0062, -0.1027,
         -0.0340, -0.1253, -0.0259, -0.0592, -0.0560, -0.0907, -0.1464, -0.0463,
          0.1049,  0.1528,  0.0797, -0.1612],
        [-0.0198,  0.2053,  0.0499, -0.2157,  0.2032,  0.1256, -0.1177,  0.0075,
          0.1134,  0.1541, -0.1628,  0.2194, -0.0046, -0.2115,  0.0491,  0.1777,
         -0.0688,  0.1293,  0.0623, -0.0709],
        [-0.2078,  0.0863,  0.0828, -0.0767,  0.0868,  0.1387,  0.1472, -0.0635,
          0.0150,  0.0866,  0.0373, -0.1962, -0.1389, -0.1193,  0.1429,  0.1355,
          0.1061, -0.0828,  0.1124,  0.0720],
        [-0.1887,  0.1563, -0.1741,  0.1724,  0.1918, -0.1936, -0.1435, -0.2214,
         -0.1774, -0.1411, -0.1292, -0.2180,  0.1671, -0.2167,  0.0380,  0.0599,
         -0.0459, -0.0304,  0.1714, -0.0170],
        [-0.1548,  0.1388, -0.1537,  0.1205, -0.1708, -0.0264,  0.0354, -0.1670,
         -0.0053, -0.0130, -0.2125, -0.1160, -0.1805, -0.0296, -0.1996,  0.0952,
         -0.1746, -0.1942, -0.1486, -0.0125],
        [-0.1085,  0.1826,  0.2199, -0.0268,  0.0250, -0.0912, -0.1112, -0.2194,
          0.0139, -0.1934, -0.0011, -0.0457, -0.0295, -0.0234,  0.1721,  0.1390,
         -0.0986, -0.1851, -0.0176,  0.0437],
        [-0.1449, -0.0984,  0.2205, -0.0105,  0.1712,  0.0673, -0.0097,  0.1138,
         -0.1579, -0.2032,  0.1014, -0.1253,  0.1125,  0.1050,  0.1062, -0.0682,
         -0.1462, -0.0919, -0.1352,  0.2221],
        [-0.1608, -0.0105,  0.0549,  0.1063,  0.1220,  0.1146, -0.0794, -0.2063,
         -0.2223, -0.0962, -0.1664,  0.0735, -0.0814, -0.1099, -0.0351,  0.0734,
         -0.2192,  0.1615, -0.1405, -0.2228],
        [-0.0624, -0.1383,  0.1137,  0.0819, -0.0649,  0.2221, -0.0834,  0.1191,
         -0.0277, -0.0652, -0.0303,  0.1926, -0.0677,  0.1382, -0.1694, -0.1661,
          0.0779,  0.1360,  0.0954, -0.0225],
        [ 0.1908, -0.0951,  0.0718, -0.1045,  0.0110,  0.1706, -0.0578, -0.1430,
         -0.1124,  0.0892, -0.0363,  0.0394,  0.1120, -0.0811, -0.0383, -0.1089,
         -0.1257, -0.1462,  0.0248,  0.1457],
        [-0.1879,  0.1829, -0.0519,  0.1905, -0.1677, -0.1947,  0.0256, -0.1909,
         -0.0324, -0.2015,  0.1950,  0.2227, -0.1998,  0.0160,  0.0716,  0.0691,
          0.0859,  0.0956,  0.0333,  0.1538],
        [-0.0043,  0.0614, -0.0818,  0.1611, -0.0010,  0.2065,  0.0035, -0.1233,
         -0.1782, -0.2233, -0.0455, -0.0940, -0.1160, -0.1678, -0.0306, -0.1478,
          0.2066, -0.1291,  0.0952, -0.2008],
        [-0.1779, -0.1329, -0.1570, -0.1554, -0.0242,  0.0706, -0.0755, -0.0404,
         -0.1998, -0.0486, -0.1280,  0.1336,  0.1186, -0.1129,  0.1585, -0.0899,
         -0.1535,  0.1768, -0.1204,  0.1906],
        [-0.1408,  0.0390,  0.0577,  0.1100,  0.0837, -0.0122,  0.1841, -0.0147,
          0.0498,  0.1759, -0.0560, -0.0524,  0.1687,  0.1950,  0.0263,  0.1136,
          0.1226,  0.0750, -0.1554, -0.1356],
        [ 0.1191, -0.1568,  0.1747,  0.1699, -0.2232,  0.0562, -0.0993,  0.1311,
         -0.1261,  0.1239, -0.0478,  0.1036, -0.1716,  0.1366, -0.1780,  0.0417,
         -0.0454,  0.2207, -0.0445, -0.1664],
        [-0.1596,  0.0301, -0.0013,  0.0723,  0.1271, -0.1360,  0.0043, -0.1577,
          0.0865,  0.1825, -0.1295,  0.0955, -0.0954, -0.0173,  0.0235,  0.0672,
          0.1268, -0.1940, -0.1577, -0.2132],
        [ 0.1608, -0.0454, -0.0125,  0.1318,  0.1673, -0.0792, -0.0986, -0.1445,
         -0.1640, -0.2106,  0.2094, -0.0525,  0.1902,  0.0033,  0.0271, -0.1770,
         -0.2159, -0.1886,  0.0084, -0.0054],
        [-0.1071,  0.1185, -0.0823,  0.1183, -0.0305,  0.2188, -0.0040,  0.1884,
         -0.0606,  0.0818,  0.2114,  0.0083, -0.0838,  0.1506,  0.0567, -0.1242,
          0.2075, -0.0759, -0.0352, -0.1646],
        [-0.1670, -0.1497,  0.1463,  0.1770,  0.1975,  0.2123,  0.2119,  0.0300,
         -0.1334, -0.1222,  0.0899, -0.1175, -0.0406,  0.2187, -0.2026,  0.0608,
          0.0453,  0.1946,  0.1498, -0.1479],
        [-0.1916, -0.1905,  0.0599,  0.0230, -0.0686,  0.0901, -0.2062,  0.0760,
         -0.2025,  0.1507, -0.0306, -0.1839,  0.1748,  0.1536, -0.1533,  0.1815,
          0.1111,  0.0917, -0.0794, -0.0867],
        [-0.1309, -0.0058,  0.1738,  0.1647, -0.1931, -0.1577,  0.1362, -0.0917,
          0.0745, -0.2113, -0.1230,  0.1415,  0.1271, -0.1184, -0.1407, -0.1444,
         -0.2227,  0.1627,  0.0954,  0.1702],
        [-0.1224,  0.0612,  0.1162,  0.2139, -0.0538,  0.0460, -0.1211, -0.0564,
          0.0476, -0.0731,  0.1279, -0.1066,  0.1870, -0.0087, -0.0702,  0.0589,
          0.2215,  0.0595, -0.2109,  0.0341],
        [-0.1073,  0.0377, -0.0789, -0.1326, -0.2125,  0.1150, -0.1472,  0.1417,
         -0.2177, -0.1977, -0.1926, -0.1346, -0.2185, -0.1378, -0.0258,  0.0850,
          0.1007, -0.2184,  0.1991,  0.0297],
        [-0.0411,  0.1137,  0.1605,  0.0381,  0.1085,  0.1436, -0.0568,  0.0891,
          0.1725, -0.1030, -0.1602,  0.0367, -0.0768, -0.0708,  0.1132,  0.0913,
         -0.2096,  0.2078, -0.1194,  0.1584],
        [ 0.0629, -0.1123,  0.0285,  0.1373,  0.1280, -0.0983, -0.1614, -0.0902,
          0.1153,  0.1410,  0.1814, -0.0304,  0.1565, -0.0855, -0.1040, -0.1925,
         -0.0860,  0.1285, -0.1063, -0.1844],
        [-0.1655, -0.2186,  0.0414, -0.0254,  0.2217, -0.0443,  0.0588,  0.1338,
          0.2222,  0.0943, -0.1618,  0.0760,  0.2052,  0.1418, -0.1351,  0.1181,
          0.1129, -0.1873, -0.1724,  0.1333]], requires_grad=True)
>>> m = Linear(20, 30)
>>> m = Linear(20, 5)
>>> m.weight
Parameter containing:
tensor([[ 0.1179,  0.1324, -0.1504, -0.0688, -0.1190, -0.2232, -0.0467, -0.0294,
         -0.1016,  0.1707,  0.1343,  0.2023,  0.0401,  0.0271,  0.1154,  0.0162,
         -0.1449,  0.1164, -0.0323, -0.1080],
        [ 0.1930,  0.1874, -0.0924, -0.0040, -0.0665,  0.0620, -0.1595,  0.1538,
          0.0026,  0.1701,  0.1065, -0.0119, -0.0273, -0.1742,  0.1927,  0.1770,
          0.1163, -0.2099,  0.0746, -0.1528],
        [ 0.1209, -0.1361,  0.1066,  0.2156, -0.1460,  0.1088,  0.0087,  0.2172,
          0.1554, -0.0536,  0.0560, -0.0636, -0.1596,  0.1757,  0.0425, -0.2160,
         -0.0042, -0.1301, -0.0116,  0.1448],
        [-0.0811,  0.0583,  0.2047,  0.1245,  0.0493,  0.1073,  0.1932, -0.2019,
          0.0090,  0.1797, -0.0720, -0.1413,  0.1382, -0.2039,  0.1087,  0.1746,
          0.0069, -0.0956,  0.0434, -0.2230],
        [-0.1081, -0.1192, -0.1290, -0.0574, -0.1846,  0.1491,  0.1121, -0.1499,
         -0.1185, -0.0579, -0.0066, -0.1308, -0.0165,  0.1301, -0.1555, -0.1820,
          0.2042,  0.1099,  0.0170,  0.1621]], requires_grad=True)
>>> m.size
Traceback (most recent call last):
  File "<pyshell#37>", line 1, in <module>
    m.size
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 539, in __getattr__
    type(self).__name__, name))
AttributeError: 'Linear' object has no attribute 'size'
>>> m.weight.size
<built-in method size of Parameter object at 0x1170a7678>
>>> m.weight.size()
torch.Size([5, 20])
>>> tensr = torch.randn(20,5)
>>> m(tensr)
Traceback (most recent call last):
  File "<pyshell#41>", line 1, in <module>
    m(tensr)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 92, in forward
    return F.linear(input, self.weight, self.bias)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py", line 1406, in linear
    ret = torch.addmm(bias, input, weight.t())
RuntimeError: size mismatch, m1: [20 x 5], m2: [20 x 5] at ../aten/src/TH/generic/THTensorMath.cpp:961
>>> m.weight
Parameter containing:
tensor([[ 0.1179,  0.1324, -0.1504, -0.0688, -0.1190, -0.2232, -0.0467, -0.0294,
         -0.1016,  0.1707,  0.1343,  0.2023,  0.0401,  0.0271,  0.1154,  0.0162,
         -0.1449,  0.1164, -0.0323, -0.1080],
        [ 0.1930,  0.1874, -0.0924, -0.0040, -0.0665,  0.0620, -0.1595,  0.1538,
          0.0026,  0.1701,  0.1065, -0.0119, -0.0273, -0.1742,  0.1927,  0.1770,
          0.1163, -0.2099,  0.0746, -0.1528],
        [ 0.1209, -0.1361,  0.1066,  0.2156, -0.1460,  0.1088,  0.0087,  0.2172,
          0.1554, -0.0536,  0.0560, -0.0636, -0.1596,  0.1757,  0.0425, -0.2160,
         -0.0042, -0.1301, -0.0116,  0.1448],
        [-0.0811,  0.0583,  0.2047,  0.1245,  0.0493,  0.1073,  0.1932, -0.2019,
          0.0090,  0.1797, -0.0720, -0.1413,  0.1382, -0.2039,  0.1087,  0.1746,
          0.0069, -0.0956,  0.0434, -0.2230],
        [-0.1081, -0.1192, -0.1290, -0.0574, -0.1846,  0.1491,  0.1121, -0.1499,
         -0.1185, -0.0579, -0.0066, -0.1308, -0.0165,  0.1301, -0.1555, -0.1820,
          0.2042,  0.1099,  0.0170,  0.1621]], requires_grad=True)
>>> tensr
tensor([[-0.5276,  1.7402,  1.5827, -0.0677,  0.9608],
        [-0.2737, -0.4586, -0.2344, -0.3081,  0.2674],
        [-0.0084, -0.8740, -1.7965,  1.3024,  0.3553],
        [-0.0234,  0.8811,  0.9480, -1.1691,  1.3910],
        [-0.0187,  0.2464,  1.3928,  1.7395,  0.3923],
        [-0.2246,  0.1459,  0.9697,  1.6229,  0.1375],
        [ 2.6425,  0.3847,  1.6094, -1.1080, -2.0968],
        [-0.5521, -0.7102,  1.3807, -0.8889, -1.2389],
        [-0.5541,  0.5545, -0.0262,  0.7113, -2.4773],
        [ 1.2918, -0.3282,  1.9572, -0.0530,  0.9872],
        [ 1.0372,  0.7712, -0.2196, -0.7411, -0.5051],
        [-0.0272,  0.7593,  0.2371, -1.0119,  0.8425],
        [-0.7221, -0.4747, -1.5061,  0.6187, -0.8371],
        [ 0.3792,  1.1313,  1.0273, -2.8994, -0.5902],
        [ 0.4413,  1.9077,  1.0589,  0.9576,  0.2011],
        [ 1.3272, -0.4695,  1.1791,  0.4139,  0.7045],
        [ 0.2178,  1.4815, -0.8387,  0.5563, -0.2061],
        [ 0.2637,  0.7376, -0.7083, -0.0798,  1.4202],
        [-0.6623, -1.6241,  0.6746, -0.1660,  2.1839],
        [ 0.6817,  0.7212,  0.0245, -0.8714,  0.5300]])
>>> tensr.size()
torch.Size([20, 5])
>>> tensr = torch.randn(1,20)
>>> m(tensr)
tensor([[-5.9738e-02,  1.3196e+00, -2.9515e-01, -1.1615e-03, -1.4517e+00]],
       grad_fn=<AddmmBackward>)
>>> tensr
tensor([[ 1.2168,  0.6772,  0.4622, -0.1677,  1.5341,  1.6725, -3.0176, -0.0689,
          0.9491,  0.1410,  0.4340,  1.0629, -0.5809, -1.5970, -0.8842,  0.5083,
         -0.3463, -0.3305, -0.8699, -0.4566]])
>>> [ 0.1179,  0.1324, -0.1504, -0.0688, -0.1190, -0.2232, -0.0467, -0.0294,
         -0.1016,  0.1707,  0.1343,  0.2023,  0.0401,  0.0271,  0.1154,  0.0162,
         -0.1449,  0.1164, -0.0323, -0.1080]
[0.1179, 0.1324, -0.1504, -0.0688, -0.119, -0.2232, -0.0467, -0.0294, -0.1016, 0.1707, 0.1343, 0.2023, 0.0401, 0.0271, 0.1154, 0.0162, -0.1449, 0.1164, -0.0323, -0.108]
>>> [ 0.1179,  0.1324, -0.1504, -0.0688, -0.1190, -0.2232, -0.0467, -0.0294,
         -0.1016,  0.1707,  0.1343,  0.2023,  0.0401,  0.0271,  0.1154,  0.0162,
         -0.1449,  0.1164, -0.0323, -0.1080]*[ 1.2168,  0.6772,  0.4622, -0.1677,  1.5341,  1.6725, -3.0176, -0.0689,
          0.9491,  0.1410,  0.4340,  1.0629, -0.5809, -1.5970, -0.8842,  0.5083,
         -0.3463, -0.3305, -0.8699, -0.4566]
Traceback (most recent call last):
  File "<pyshell#49>", line 5, in <module>
    -0.3463, -0.3305, -0.8699, -0.4566]
TypeError: can't multiply sequence by non-int of type 'list'
>>> import numpy as np
>>> a = np.array([ 0.1179,  0.1324, -0.1504, -0.0688, -0.1190, -0.2232, -0.0467, -0.0294,
         -0.1016,  0.1707,  0.1343,  0.2023,  0.0401,  0.0271,  0.1154,  0.0162,
         -0.1449,  0.1164, -0.0323, -0.1080])
>>> b = np.array([ 1.2168,  0.6772,  0.4622, -0.1677,  1.5341,  1.6725, -3.0176, -0.0689,
          0.9491,  0.1410,  0.4340,  1.0629, -0.5809, -1.5970, -0.8842,  0.5083,
         -0.3463, -0.3305, -0.8699, -0.4566])
>>> a*b
array([ 0.14346072,  0.08966128, -0.06951488,  0.01153776, -0.1825579 ,
       -0.373302  ,  0.14092192,  0.00202566, -0.09642856,  0.0240687 ,
        0.0582862 ,  0.21502467, -0.02329409, -0.0432787 , -0.10203668,
        0.00823446,  0.05017887, -0.0384702 ,  0.02809777,  0.0493128 ])
>>> (a*b).sum()
-0.10807220000000015
>>> m.weight
Parameter containing:
tensor([[ 0.1179,  0.1324, -0.1504, -0.0688, -0.1190, -0.2232, -0.0467, -0.0294,
         -0.1016,  0.1707,  0.1343,  0.2023,  0.0401,  0.0271,  0.1154,  0.0162,
         -0.1449,  0.1164, -0.0323, -0.1080],
        [ 0.1930,  0.1874, -0.0924, -0.0040, -0.0665,  0.0620, -0.1595,  0.1538,
          0.0026,  0.1701,  0.1065, -0.0119, -0.0273, -0.1742,  0.1927,  0.1770,
          0.1163, -0.2099,  0.0746, -0.1528],
        [ 0.1209, -0.1361,  0.1066,  0.2156, -0.1460,  0.1088,  0.0087,  0.2172,
          0.1554, -0.0536,  0.0560, -0.0636, -0.1596,  0.1757,  0.0425, -0.2160,
         -0.0042, -0.1301, -0.0116,  0.1448],
        [-0.0811,  0.0583,  0.2047,  0.1245,  0.0493,  0.1073,  0.1932, -0.2019,
          0.0090,  0.1797, -0.0720, -0.1413,  0.1382, -0.2039,  0.1087,  0.1746,
          0.0069, -0.0956,  0.0434, -0.2230],
        [-0.1081, -0.1192, -0.1290, -0.0574, -0.1846,  0.1491,  0.1121, -0.1499,
         -0.1185, -0.0579, -0.0066, -0.1308, -0.0165,  0.1301, -0.1555, -0.1820,
          0.2042,  0.1099,  0.0170,  0.1621]], requires_grad=True)
>>> tensr
tensor([[ 1.2168,  0.6772,  0.4622, -0.1677,  1.5341,  1.6725, -3.0176, -0.0689,
          0.9491,  0.1410,  0.4340,  1.0629, -0.5809, -1.5970, -0.8842,  0.5083,
         -0.3463, -0.3305, -0.8699, -0.4566]])
>>> m(tensr)
tensor([[-5.9738e-02,  1.3196e+00, -2.9515e-01, -1.1615e-03, -1.4517e+00]],
       grad_fn=<AddmmBackward>)
>>> a = np.array([[ 0.1179,  0.1324, -0.1504, -0.0688, -0.1190, -0.2232, -0.0467, -0.0294,
         -0.1016,  0.1707,  0.1343,  0.2023,  0.0401,  0.0271,  0.1154,  0.0162,
         -0.1449,  0.1164, -0.0323, -0.1080],
        [ 0.1930,  0.1874, -0.0924, -0.0040, -0.0665,  0.0620, -0.1595,  0.1538,
          0.0026,  0.1701,  0.1065, -0.0119, -0.0273, -0.1742,  0.1927,  0.1770,
          0.1163, -0.2099,  0.0746, -0.1528],
        [ 0.1209, -0.1361,  0.1066,  0.2156, -0.1460,  0.1088,  0.0087,  0.2172,
          0.1554, -0.0536,  0.0560, -0.0636, -0.1596,  0.1757,  0.0425, -0.2160,
         -0.0042, -0.1301, -0.0116,  0.1448],
        [-0.0811,  0.0583,  0.2047,  0.1245,  0.0493,  0.1073,  0.1932, -0.2019,
          0.0090,  0.1797, -0.0720, -0.1413,  0.1382, -0.2039,  0.1087,  0.1746,
          0.0069, -0.0956,  0.0434, -0.2230],
        [-0.1081, -0.1192, -0.1290, -0.0574, -0.1846,  0.1491,  0.1121, -0.1499,
         -0.1185, -0.0579, -0.0066, -0.1308, -0.0165,  0.1301, -0.1555, -0.1820,
          0.2042,  0.1099,  0.0170,  0.1621]])
>>> b = np.array([[ 1.2168,  0.6772,  0.4622, -0.1677,  1.5341,  1.6725, -3.0176, -0.0689,
          0.9491,  0.1410,  0.4340,  1.0629, -0.5809, -1.5970, -0.8842,  0.5083,
         -0.3463, -0.3305, -0.8699, -0.4566]])
>>> a*b
array([[ 0.14346072,  0.08966128, -0.06951488,  0.01153776, -0.1825579 ,
        -0.373302  ,  0.14092192,  0.00202566, -0.09642856,  0.0240687 ,
         0.0582862 ,  0.21502467, -0.02329409, -0.0432787 , -0.10203668,
         0.00823446,  0.05017887, -0.0384702 ,  0.02809777,  0.0493128 ],
       [ 0.2348424 ,  0.12690728, -0.04270728,  0.0006708 , -0.10201765,
         0.103695  ,  0.4813072 , -0.01059682,  0.00246766,  0.0239841 ,
         0.046221  , -0.01264851,  0.01585857,  0.2781974 , -0.17038534,
         0.0899691 , -0.04027469,  0.06937195, -0.06489454,  0.06976848],
       [ 0.14711112, -0.09216692,  0.04927052, -0.03615612, -0.2239786 ,
         0.181968  , -0.02625312, -0.01496508,  0.14749014, -0.0075576 ,
         0.024304  , -0.06760044,  0.09271164, -0.2805929 , -0.0375785 ,
        -0.1097928 ,  0.00145446,  0.04299805,  0.01009084, -0.06611568],
       [-0.09868248,  0.03948076,  0.09461234, -0.02087865,  0.07563113,
         0.17945925, -0.58300032,  0.01391091,  0.0085419 ,  0.0253377 ,
        -0.031248  , -0.15018777, -0.08028038,  0.3256283 , -0.09611254,
         0.08874918, -0.00238947,  0.0315958 , -0.03775366,  0.1018218 ],
       [-0.13153608, -0.08072224, -0.0596238 ,  0.00962598, -0.28319486,
         0.24936975, -0.33827296,  0.01032811, -0.11246835, -0.0081639 ,
        -0.0028644 , -0.13902732,  0.00958485, -0.2077697 ,  0.1374931 ,
        -0.0925106 , -0.07071446, -0.03632195, -0.0147883 , -0.07401486]])
>>> np.dot(a, b)
Traceback (most recent call last):
  File "<pyshell#61>", line 1, in <module>
    np.dot(a, b)
ValueError: shapes (5,20) and (1,20) not aligned: 20 (dim 1) != 1 (dim 0)
>>> np.dot(a, b.T)
array([[-0.1080722 ],
       [ 1.09973611],
       [-0.26535899],
       [-0.1157642 ],
       [-1.23559199]])
>>> np.dot(b, a)
Traceback (most recent call last):
  File "<pyshell#63>", line 1, in <module>
    np.dot(b, a)
ValueError: shapes (1,20) and (5,20) not aligned: 20 (dim 1) != 5 (dim 0)
>>> np.dot(b, a.T)
array([[-0.1080722 ,  1.09973611, -0.26535899, -0.1157642 , -1.23559199]])
>>> np.dot(a.T, b)
Traceback (most recent call last):
  File "<pyshell#65>", line 1, in <module>
    np.dot(a.T, b)
ValueError: shapes (20,5) and (1,20) not aligned: 5 (dim 1) != 1 (dim 0)
>>> np.dot(a.T, b.T)
Traceback (most recent call last):
  File "<pyshell#66>", line 1, in <module>
    np.dot(a.T, b.T)
ValueError: shapes (20,5) and (20,1) not aligned: 5 (dim 1) != 20 (dim 0)
>>> np.dot(b.T, a)
Traceback (most recent call last):
  File "<pyshell#67>", line 1, in <module>
    np.dot(b.T, a)
ValueError: shapes (20,1) and (5,20) not aligned: 1 (dim 1) != 5 (dim 0)
>>> np.dot(b, a.T)
array([[-0.1080722 ,  1.09973611, -0.26535899, -0.1157642 , -1.23559199]])
>>> m.weight
Parameter containing:
tensor([[ 0.1179,  0.1324, -0.1504, -0.0688, -0.1190, -0.2232, -0.0467, -0.0294,
         -0.1016,  0.1707,  0.1343,  0.2023,  0.0401,  0.0271,  0.1154,  0.0162,
         -0.1449,  0.1164, -0.0323, -0.1080],
        [ 0.1930,  0.1874, -0.0924, -0.0040, -0.0665,  0.0620, -0.1595,  0.1538,
          0.0026,  0.1701,  0.1065, -0.0119, -0.0273, -0.1742,  0.1927,  0.1770,
          0.1163, -0.2099,  0.0746, -0.1528],
        [ 0.1209, -0.1361,  0.1066,  0.2156, -0.1460,  0.1088,  0.0087,  0.2172,
          0.1554, -0.0536,  0.0560, -0.0636, -0.1596,  0.1757,  0.0425, -0.2160,
         -0.0042, -0.1301, -0.0116,  0.1448],
        [-0.0811,  0.0583,  0.2047,  0.1245,  0.0493,  0.1073,  0.1932, -0.2019,
          0.0090,  0.1797, -0.0720, -0.1413,  0.1382, -0.2039,  0.1087,  0.1746,
          0.0069, -0.0956,  0.0434, -0.2230],
        [-0.1081, -0.1192, -0.1290, -0.0574, -0.1846,  0.1491,  0.1121, -0.1499,
         -0.1185, -0.0579, -0.0066, -0.1308, -0.0165,  0.1301, -0.1555, -0.1820,
          0.2042,  0.1099,  0.0170,  0.1621]], requires_grad=True)
>>> m = Linear(2,3)
>>> m.weight
Parameter containing:
tensor([[-0.6482,  0.5176],
        [ 0.6895, -0.1695],
        [-0.5813,  0.4666]], requires_grad=True)
>>> tensr = torch.randn(1,2)
>>> m(tensr)
tensor([[-0.4487,  0.3239,  0.1389]], grad_fn=<AddmmBackward>)
>>> tensr
tensor([[-0.1518, -0.0703]])
>>> np.([-0.6482,  0.5176])
SyntaxError: invalid syntax
>>> np.array([-0.6482,  0.5176]) * np.array([-0.1518, -0.0703])
array([ 0.09839676, -0.03638728])
>>> tensr = torch.tensor([1,2])
>>> m(tensr)
Traceback (most recent call last):
  File "<pyshell#78>", line 1, in <module>
    m(tensr)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 92, in forward
    return F.linear(input, self.weight, self.bias)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py", line 1408, in linear
    output = input.matmul(weight.t())
RuntimeError: Expected object of scalar type Long but got scalar type Float for argument #2 'mat2'
>>> tensr = torch.tensor([[1,2]])
>>> m(tensr)
Traceback (most recent call last):
  File "<pyshell#80>", line 1, in <module>
    m(tensr)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 92, in forward
    return F.linear(input, self.weight, self.bias)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py", line 1406, in linear
    ret = torch.addmm(bias, input, weight.t())
RuntimeError: Expected object of scalar type Float but got scalar type Long for argument #4 'mat1'
>>> tensr
tensor([[1, 2]])
>>> tensr = torch.randn(1,2)
>>> tensr
tensor([[ 0.0823, -1.5137]])
>>> m(tensr)
tensor([[-1.3476,  0.7300, -0.6706]], grad_fn=<AddmmBackward>)
>>> tensor([[1, 2]])
Traceback (most recent call last):
  File "<pyshell#85>", line 1, in <module>
    tensor([[1, 2]])
NameError: name 'tensor' is not defined
>>> tensr = torch.tensor([[1,2]])
>>> tensr
tensor([[1, 2]])
>>> m(tensr)
Traceback (most recent call last):
  File "<pyshell#88>", line 1, in <module>
    m(tensr)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 92, in forward
    return F.linear(input, self.weight, self.bias)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py", line 1406, in linear
    ret = torch.addmm(bias, input, weight.t())
RuntimeError: Expected object of scalar type Float but got scalar type Long for argument #4 'mat1'
>>> tensr = torch.tensor([[1,1]])
>>> m(tensr)
Traceback (most recent call last):
  File "<pyshell#90>", line 1, in <module>
    m(tensr)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 92, in forward
    return F.linear(input, self.weight, self.bias)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py", line 1406, in linear
    ret = torch.addmm(bias, input, weight.t())
RuntimeError: Expected object of scalar type Float but got scalar type Long for argument #4 'mat1'
>>> tensr = torch.tensor([[0.1,0,1]])
>>> m(tensr)
Traceback (most recent call last):
  File "<pyshell#92>", line 1, in <module>
    m(tensr)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 92, in forward
    return F.linear(input, self.weight, self.bias)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py", line 1406, in linear
    ret = torch.addmm(bias, input, weight.t())
RuntimeError: size mismatch, m1: [1 x 3], m2: [2 x 3] at ../aten/src/TH/generic/THTensorMath.cpp:961
>>> tensr = torch.tensor([[0.1,0.1]])
>>> m(tensr)
tensor([[-0.5238,  0.4687,  0.0720]], grad_fn=<AddmmBackward>)
>>> m.weight
Parameter containing:
tensor([[-0.6482,  0.5176],
        [ 0.6895, -0.1695],
        [-0.5813,  0.4666]], requires_grad=True)
>>> m.weight.m = Linear(2,3, bias=False)
>>> m.weight
Parameter containing:
tensor([[-0.6482,  0.5176],
        [ 0.6895, -0.1695],
        [-0.5813,  0.4666]], requires_grad=True)
>>> tensr = torch.tensor([[0.1,0.1]])
>>> m(tensr)
tensor([[-0.5238,  0.4687,  0.0720]], grad_fn=<AddmmBackward>)
>>> [[-0.6482,  0.5176],
        [ 0.6895, -0.1695],
        [-0.5813,  0.4666]].T
Traceback (most recent call last):
  File "<pyshell#100>", line 3, in <module>
    [-0.5813,  0.4666]].T
AttributeError: 'list' object has no attribute 'T'
>>> np.array([[-0.6482,  0.5176],
        [ 0.6895, -0.1695],
        [-0.5813,  0.4666]])
array([[-0.6482,  0.5176],
       [ 0.6895, -0.1695],
       [-0.5813,  0.4666]])
>>> m =np.array([[-0.6482,  0.5176],
        [ 0.6895, -0.1695],
        [-0.5813,  0.4666]])
>>> m
array([[-0.6482,  0.5176],
       [ 0.6895, -0.1695],
       [-0.5813,  0.4666]])
>>> m.T
array([[-0.6482,  0.6895, -0.5813],
       [ 0.5176, -0.1695,  0.4666]])
>>> np.dot(tensr, m.T)
array([[-0.01306,  0.052  , -0.01147]])
>>> m(tensr)
Traceback (most recent call last):
  File "<pyshell#106>", line 1, in <module>
    m(tensr)
TypeError: 'numpy.ndarray' object is not callable
>>> m.weight.m = Linear(2,3, bias=False)
Traceback (most recent call last):
  File "<pyshell#107>", line 1, in <module>
    m.weight.m = Linear(2,3, bias=False)
AttributeError: 'numpy.ndarray' object has no attribute 'weight'
>>> m = Linear(2,3, bias=False)
>>> m
Linear(in_features=2, out_features=3, bias=False)
>>> m.weight.data
tensor([[-0.3444, -0.2019],
        [-0.5648, -0.0184],
        [ 0.3353,  0.2161]])
>>> m.weight.abs
<built-in method abs of Parameter object at 0x1170a7288>
>>> m.weight.abs()
tensor([[0.3444, 0.2019],
        [0.5648, 0.0184],
        [0.3353, 0.2161]], grad_fn=<AbsBackward>)
>>> m.weight.
SyntaxError: invalid syntax
>>> m.weight
Parameter containing:
tensor([[-0.3444, -0.2019],
        [-0.5648, -0.0184],
        [ 0.3353,  0.2161]], requires_grad=True)
>>> m(tensr)
tensor([[-0.0546, -0.0583,  0.0551]], grad_fn=<MmBackward>)
>>> m =np.array([[-0.3444, -0.2019],
        [-0.5648, -0.0184],
        [ 0.3353,  0.2161]])
>>> np.dot(tensr, m.T)
array([[-0.05463, -0.05832,  0.05514]])
>>> from torch import nn
>>> nn.MSELoss
<class 'torch.nn.modules.loss.MSELoss'>
>>> l = nn.MSELoss()
>>> a = torch.tensor([1,0,0,0,0])
>>> b = torch.tensor([0.05,0.2,0.6,0.1,0.05])
>>> l(a,b)
Traceback (most recent call last):
  File "<pyshell#123>", line 1, in <module>
    l(a,b)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 443, in forward
    return F.mse_loss(input, target, reduction=self.reduction)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py", line 2257, in mse_loss
    ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
RuntimeError: _thnn_mse_loss_forward not supported on CPUType for Long
>>> l
MSELoss()
>>> 
>>> l(b)
Traceback (most recent call last):
  File "<pyshell#126>", line 1, in <module>
    l(b)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
TypeError: forward() missing 1 required positional argument: 'target'
>>> l(b, )
Traceback (most recent call last):
  File "<pyshell#127>", line 1, in <module>
    l(b, )
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
TypeError: forward() missing 1 required positional argument: 'target'
a
>>> l(b, a)
Traceback (most recent call last):
  File "<pyshell#128>", line 1, in <module>
    l(b, a)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 443, in forward
    return F.mse_loss(input, target, reduction=self.reduction)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py", line 2257, in mse_loss
    ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
RuntimeError: Expected object of scalar type Float but got scalar type Long for argument #2 'target'
>>> l(b, a)
Traceback (most recent call last):
  File "<pyshell#129>", line 1, in <module>
    l(b, a)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 443, in forward
    return F.mse_loss(input, target, reduction=self.reduction)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py", line 2257, in mse_loss
    ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
RuntimeError: Expected object of scalar type Float but got scalar type Long for argument #2 'target'
>>> l = nn.MSELoss().cuda()
>>> l(b, a)
Traceback (most recent call last):
  File "<pyshell#131>", line 1, in <module>
    l(b, a)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 443, in forward
    return F.mse_loss(input, target, reduction=self.reduction)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py", line 2257, in mse_loss
    ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
RuntimeError: Expected object of scalar type Float but got scalar type Long for argument #2 'target'
>>> l(a,b)
Traceback (most recent call last):
  File "<pyshell#132>", line 1, in <module>
    l(a,b)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 443, in forward
    return F.mse_loss(input, target, reduction=self.reduction)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py", line 2257, in mse_loss
    ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
RuntimeError: _thnn_mse_loss_forward not supported on CPUType for Long
>>> a
tensor([1, 0, 0, 0, 0])
>>> b
tensor([0.0500, 0.2000, 0.6000, 0.1000, 0.0500])
>>> a.size
<built-in method size of Tensor object at 0x1170a71f8>
>>> a.size()
torch.Size([5])
>>> b.size()
torch.Size([5])
>>> a = torch.tensor([[1,0,0,0,0]])
>>> b = torch.tensor([[0.05,0.2,0.6,0.1,0.05]])
>>> l(a,b)
Traceback (most recent call last):
  File "<pyshell#140>", line 1, in <module>
    l(a,b)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 443, in forward
    return F.mse_loss(input, target, reduction=self.reduction)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py", line 2257, in mse_loss
    ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
RuntimeError: _thnn_mse_loss_forward not supported on CPUType for Long
>>> loss = l(a,b)
Traceback (most recent call last):
  File "<pyshell#141>", line 1, in <module>
    loss = l(a,b)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 443, in forward
    return F.mse_loss(input, target, reduction=self.reduction)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py", line 2257, in mse_loss
    ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
RuntimeError: _thnn_mse_loss_forward not supported on CPUType for Long
>>> input = torch.randn(3, 5, requires_grad=True)
>>> target = torch.randn(3, 5)
>>> output = l(input, target)
>>> output
tensor(1.3588, grad_fn=<MseLossBackward>)
>>> input
tensor([[ 1.4179, -0.8598,  0.0183, -0.7076,  0.8688],
        [-2.6949,  1.4950, -0.5087, -0.1113, -0.1004],
        [ 1.2820,  0.6034,  0.3325, -1.1466, -0.8563]], requires_grad=True)
>>> target
tensor([[-0.1871, -0.4766,  0.7427,  0.7845,  1.1301],
        [ 0.2827,  0.9635,  0.3363, -0.0847, -1.2024],
        [-0.2603,  0.5340, -0.6868, -0.7074, -0.4734]])
>>> input - targte
Traceback (most recent call last):
  File "<pyshell#148>", line 1, in <module>
    input - targte
NameError: name 'targte' is not defined
>>> input - target
tensor([[ 1.6050, -0.3832, -0.7243, -1.4921, -0.2612],
        [-2.9776,  0.5315, -0.8450, -0.0266,  1.1020],
        [ 1.5423,  0.0694,  1.0193, -0.4392, -0.3829]], grad_fn=<SubBackward0>)
>>> (input - target)**2
tensor([[2.5759e+00, 1.4685e-01, 5.2462e-01, 2.2264e+00, 6.8238e-02],
        [8.8660e+00, 2.8246e-01, 7.1402e-01, 7.0693e-04, 1.2145e+00],
        [2.3786e+00, 4.8176e-03, 1.0389e+00, 1.9288e-01, 1.4664e-01]],
       grad_fn=<PowBackward0>)
>>> ((input - target)**2).sum()
tensor(20.3815, grad_fn=<SumBackward0>)
>>> ((input - target)**2).sum(1)
tensor([ 5.5420, 11.0777,  3.7618], grad_fn=<SumBackward2>)
>>> aaa = ((input - target)**2).sum(1)
>>> aaa.mean()
tensor(6.7938, grad_fn=<MeanBackward0>)
>>> ((input - target)**2).mean(	)
tensor(1.3588, grad_fn=<MeanBackward0>)
>>> ((input - target)**2).mean(1)
tensor([1.1084, 2.2155, 0.7524], grad_fn=<MeanBackward2>)
>>> aaa = ((input - target)**2).mean(1)
>>> aaa
tensor([1.1084, 2.2155, 0.7524], grad_fn=<MeanBackward2>)
>>> aaa.mean()
tensor(1.3588, grad_fn=<MeanBackward0>)
>>> t = nn.Tanh()
>>> t(a)
Traceback (most recent call last):
  File "<pyshell#161>", line 1, in <module>
    t(a)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/activation.py", line 309, in forward
    return torch.tanh(input)
RuntimeError: tanh_vml_cpu not implemented for 'Long'
>>> t(input)
tensor([[ 0.8892, -0.6962,  0.0183, -0.6092,  0.7008],
        [-0.9909,  0.9042, -0.4689, -0.1108, -0.1001],
        [ 0.8570,  0.5394,  0.3207, -0.8166, -0.6944]], grad_fn=<TanhBackward>)
>>> aaa = t(input)
>>> aaa.sum()
tensor(-0.2574, grad_fn=<SumBackward0>)
>>> aaa.sum(	)
tensor(-0.2574, grad_fn=<SumBackward0>)
>>> aaa.sum(1)
tensor([ 0.3029, -0.7665,  0.2062], grad_fn=<SumBackward2>)
>>> t(target)
tensor([[-0.1849, -0.4435,  0.6307,  0.6553,  0.8110],
        [ 0.2754,  0.7458,  0.3241, -0.0845, -0.8344],
        [-0.2546,  0.4884, -0.5959, -0.6090, -0.4409]])
>>> input
tensor([[ 1.4179, -0.8598,  0.0183, -0.7076,  0.8688],
        [-2.6949,  1.4950, -0.5087, -0.1113, -0.1004],
        [ 1.2820,  0.6034,  0.3325, -1.1466, -0.8563]], requires_grad=True)
>>> nn.Tanh()
Tanh()
>>> nn.Tanh(1.4179)
Traceback (most recent call last):
  File "<pyshell#170>", line 1, in <module>
    nn.Tanh(1.4179)
TypeError: __init__() takes 1 positional argument but 2 were given
>>> nn.Tanh()(1.4179)
Traceback (most recent call last):
  File "<pyshell#171>", line 1, in <module>
    nn.Tanh()(1.4179)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/activation.py", line 309, in forward
    return torch.tanh(input)
TypeError: tanh(): argument 'input' (position 1) must be Tensor, not float
>>> nn.Tanh()([1.4179])
Traceback (most recent call last):
  File "<pyshell#172>", line 1, in <module>
    nn.Tanh()([1.4179])
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/activation.py", line 309, in forward
    return torch.tanh(input)
TypeError: tanh(): argument 'input' (position 1) must be Tensor, not list
>>> np
<module 'numpy' from '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/__init__.py'>
>>> x = 1.4179
>>> (np.e**(x)-np.e**(-x))/(np.e**(x)+np.e**(-x))
0.8891600162469427
>>> def y(x): (np.e**(x)-np.e**(-x))

>>> y(x)
>>> y(x)
>>> x
1.4179
>>> y(x)
>>> def y(x): return (np.e**(x)-np.e**(-x))

>>> y((np.e**(x)-np.e**(-x)))
48.70580295146569
>>> y(x)
3.8862194536099315
>>> def y(x):  return (np.e**(x)-np.e**(-x))/(np.e**(x)+np.e**(-x))

>>> y(x)
0.8891600162469427
>>> 
